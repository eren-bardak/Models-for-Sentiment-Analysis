{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldfrKsqM1zml"
      },
      "source": [
        "\n",
        "**Task Description:**\n",
        "You have learned about transformers and their applications in natural language processing. In this assignment, you will apply your knowledge by implementing a transformer-based model to solve a text classification task.\n",
        "\n",
        "\n",
        "**Dataset:**\n",
        "You will be using the IMDB movie review dataset, which contains movie reviews labeled as positive or negative sentiment. The dataset will be downloaded and loaded using Python's file handling capabilities.\n",
        "\n",
        "**Task:**\n",
        "\n",
        "Your task is to build a transformer-based model using the torch.nn.Transformer module to classify movie reviews as positive or negative sentiment. You can use the provided dataset for training and evaluation.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "(1) Download and Extract the IMDB Dataset:Run the following script to download and extract the IMDB dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FZnKJoXqu31t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Function to download and extract IMDB dataset\n",
        "def download_extract_imdb(root=\"./imdb_data\"):\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "\n",
        "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    filename = os.path.join(root, \"aclImdb_v1.tar.gz\")\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    # Extract the tar.gz file\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall(root)\n",
        "\n",
        "# Download and extract IMDB dataset\n",
        "download_extract_imdb()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlZts1t91u0m"
      },
      "source": [
        "(2) Load and Preprocess the Dataset:Use the following script to load the IMDB dataset, preprocess it, and tokenize the reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p577rCQPu67M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Set up tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Load training data\n",
        "def load_imdb_data(root=\"./imdb_data/aclImdb\"):\n",
        "    train_data = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        label_dir = os.path.join(root, \"train\", label)\n",
        "        for filename in os.listdir(label_dir):\n",
        "            with open(os.path.join(label_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                review = file.read()\n",
        "                # Tokenize review\n",
        "                tokenized_review = tokenizer(review)\n",
        "                train_data.append((tokenized_review, 1 if label == \"pos\" else 0))\n",
        "    return train_data\n",
        "\n",
        "# Load training data\n",
        "train_data = load_imdb_data()\n",
        "\n",
        "# Load testing data\n",
        "def load_test_data(root=\"./imdb_data/aclImdb\"):\n",
        "    test_data = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        label_dir = os.path.join(root, \"test\", label)\n",
        "        for filename in os.listdir(label_dir):\n",
        "            with open(os.path.join(label_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                review = file.read()\n",
        "                # Tokenize review\n",
        "                tokenized_review = tokenizer(review)\n",
        "                test_data.append((tokenized_review, 1 if label == \"pos\" else 0))\n",
        "    return test_data\n",
        "\n",
        "# Load testing data\n",
        "test_data = load_test_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fic4RUKvg2_",
        "outputId": "b18fe37a-84bb-4643-c51c-36bbb6966959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Positive Example:\n",
            "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.']\n",
            "Tokenized Negative Example:\n",
            "['working', 'with', 'one', 'of', 'the', 'best', 'shakespeare', 'sources', ',', 'this', 'film', 'manages', 'to', 'be', 'creditable', 'to', 'it', \"'\", 's', 'source', ',', 'whilst', 'still', 'appealing', 'to', 'a', 'wider', 'audience', '.', 'branagh', 'steals', 'the', 'film', 'from', 'under', 'fishburne', \"'\", 's', 'nose', ',', 'and', 'there', \"'\", 's', 'a', 'talented', 'cast', 'on', 'good', 'form', '.']\n"
          ]
        }
      ],
      "source": [
        "# Display tokenized positive and negative examples\n",
        "print(\"Tokenized Positive Example:\")\n",
        "print(train_data[0][0])\n",
        "print(\"Tokenized Negative Example:\")\n",
        "print(train_data[len(train_data)//2][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgp2OksU2KfU",
        "outputId": "2968d124-e86c-495c-d1ab-7dc5591dd1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset:\n",
            "Label: Positive\n",
            "Tokenized Review: ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.']\n",
            "\n",
            "Label: Positive\n",
            "Tokenized Review: ['bizarre', 'horror', 'movie', 'filled', 'with', 'famous', 'faces', 'but', 'stolen', 'by', 'cristina', 'raines', '(', 'later', 'of', 'tv', \"'\", 's', 'flamingo', 'road', ')', 'as', 'a', 'pretty', 'but', 'somewhat', 'unstable', 'model', 'with', 'a', 'gummy', 'smile', 'who', 'is', 'slated', 'to', 'pay', 'for', 'her', 'attempted', 'suicides', 'by', 'guarding', 'the', 'gateway', 'to', 'hell', '!', 'the', 'scenes', 'with', 'raines', 'modeling', 'are', 'very', 'well', 'captured', ',', 'the', 'mood', 'music', 'is', 'perfect', ',', 'deborah', 'raffin', 'is', 'charming', 'as', 'cristina', \"'\", 's', 'pal', ',', 'but', 'when', 'raines', 'moves', 'into', 'a', 'creepy', 'brooklyn', 'heights', 'brownstone', '(', 'inhabited', 'by', 'a', 'blind', 'priest', 'on', 'the', 'top', 'floor', ')', ',', 'things', 'really', 'start', 'cooking', '.', 'the', 'neighbors', ',', 'including', 'a', 'fantastically', 'wicked', 'burgess', 'meredith', 'and', 'kinky', 'couple', 'sylvia', 'miles', '&', 'beverly', 'd', \"'\", 'angelo', ',', 'are', 'a', 'diabolical', 'lot', ',', 'and', 'eli', 'wallach', 'is', 'great', 'fun', 'as', 'a', 'wily', 'police', 'detective', '.', 'the', 'movie', 'is', 'nearly', 'a', 'cross-pollination', 'of', 'rosemary', \"'\", 's', 'baby', 'and', 'the', 'exorcist--but', 'what', 'a', 'combination', '!', 'based', 'on', 'the', 'best-seller', 'by', 'jeffrey', 'konvitz', ',', 'the', 'sentinel', 'is', 'entertainingly', 'spooky', ',', 'full', 'of', 'shocks', 'brought', 'off', 'well', 'by', 'director', 'michael', 'winner', ',', 'who', 'mounts', 'a', 'thoughtfully', 'downbeat', 'ending', 'with', 'skill', '.', '***1/2', 'from', '****']\n",
            "\n",
            "Label: Positive\n",
            "Tokenized Review: ['a', 'solid', ',', 'if', 'unremarkable', 'film', '.', 'matthau', ',', 'as', 'einstein', ',', 'was', 'wonderful', '.', 'my', 'favorite', 'part', ',', 'and', 'the', 'only', 'thing', 'that', 'would', 'make', 'me', 'go', 'out', 'of', 'my', 'way', 'to', 'see', 'this', 'again', ',', 'was', 'the', 'wonderful', 'scene', 'with', 'the', 'physicists', 'playing', 'badmitton', ',', 'i', 'loved', 'the', 'sweaters', 'and', 'the', 'conversation', 'while', 'they', 'waited', 'for', 'robbins', 'to', 'retrieve', 'the', 'birdie', '.']\n",
            "\n",
            "Testing Dataset:\n",
            "Label: Positive\n",
            "Tokenized Review: ['based', 'on', 'an', 'actual', 'story', ',', 'john', 'boorman', 'shows', 'the', 'struggle', 'of', 'an', 'american', 'doctor', ',', 'whose', 'husband', 'and', 'son', 'were', 'murdered', 'and', 'she', 'was', 'continually', 'plagued', 'with', 'her', 'loss', '.', 'a', 'holiday', 'to', 'burma', 'with', 'her', 'sister', 'seemed', 'like', 'a', 'good', 'idea', 'to', 'get', 'away', 'from', 'it', 'all', ',', 'but', 'when', 'her', 'passport', 'was', 'stolen', 'in', 'rangoon', ',', 'she', 'could', 'not', 'leave', 'the', 'country', 'with', 'her', 'sister', ',', 'and', 'was', 'forced', 'to', 'stay', 'back', 'until', 'she', 'could', 'get', 'i', '.', 'd', '.', 'papers', 'from', 'the', 'american', 'embassy', '.', 'to', 'fill', 'in', 'a', 'day', 'before', 'she', 'could', 'fly', 'out', ',', 'she', 'took', 'a', 'trip', 'into', 'the', 'countryside', 'with', 'a', 'tour', 'guide', '.', 'i', 'tried', 'finding', 'something', 'in', 'those', 'stone', 'statues', ',', 'but', 'nothing', 'stirred', 'in', 'me', '.', 'i', 'was', 'stone', 'myself', '.', 'suddenly', 'all', 'hell', 'broke', 'loose', 'and', 'she', 'was', 'caught', 'in', 'a', 'political', 'revolt', '.', 'just', 'when', 'it', 'looked', 'like', 'she', 'had', 'escaped', 'and', 'safely', 'boarded', 'a', 'train', ',', 'she', 'saw', 'her', 'tour', 'guide', 'get', 'beaten', 'and', 'shot', '.', 'in', 'a', 'split', 'second', 'she', 'decided', 'to', 'jump', 'from', 'the', 'moving', 'train', 'and', 'try', 'to', 'rescue', 'him', ',', 'with', 'no', 'thought', 'of', 'herself', '.', 'continually', 'her', 'life', 'was', 'in', 'danger', '.', 'here', 'is', 'a', 'woman', 'who', 'demonstrated', 'spontaneous', ',', 'selfless', 'charity', ',', 'risking', 'her', 'life', 'to', 'save', 'another', '.', 'patricia', 'arquette', 'is', 'beautiful', ',', 'and', 'not', 'just', 'to', 'look', 'at', 'she', 'has', 'a', 'beautiful', 'heart', '.', 'this', 'is', 'an', 'unforgettable', 'story', '.', 'we', 'are', 'taught', 'that', 'suffering', 'is', 'the', 'one', 'promise', 'that', 'life', 'always', 'keeps', '.']\n",
            "\n",
            "Label: Positive\n",
            "Tokenized Review: ['this', 'is', 'a', 'gem', '.', 'as', 'a', 'film', 'four', 'production', '-', 'the', 'anticipated', 'quality', 'was', 'indeed', 'delivered', '.', 'shot', 'with', 'great', 'style', 'that', 'reminded', 'me', 'some', 'errol', 'morris', 'films', ',', 'well', 'arranged', 'and', 'simply', 'gripping', '.', 'it', \"'\", 's', 'long', 'yet', 'horrifying', 'to', 'the', 'point', 'it', \"'\", 's', 'excruciating', '.', 'we', 'know', 'something', 'bad', 'happened', '(', 'one', 'can', 'guess', 'by', 'the', 'lack', 'of', 'participation', 'of', 'a', 'person', 'in', 'the', 'interviews', ')', 'but', 'we', 'are', 'compelled', 'to', 'see', 'it', ',', 'a', 'bit', 'like', 'a', 'car', 'accident', 'in', 'slow', 'motion', '.', 'the', 'story', 'spans', 'most', 'conceivable', 'aspects', 'and', 'unlike', 'some', 'documentaries', 'did', 'not', 'try', 'and', 'refrain', 'from', 'showing', 'the', 'grimmer', 'sides', 'of', 'the', 'stories', ',', 'as', 'also', 'dealing', 'with', 'the', 'guilt', 'of', 'the', 'people', 'don', 'left', 'behind', 'him', ',', 'wondering', 'why', 'they', 'didn', \"'\", 't', 'stop', 'him', 'in', 'time', '.', 'it', 'took', 'me', 'a', 'few', 'hours', 'to', 'get', 'out', 'of', 'the', 'melancholy', 'that', 'gripped', 'me', 'after', 'seeing', 'this', 'very-well', 'made', 'documentary', '.']\n",
            "\n",
            "Label: Positive\n",
            "Tokenized Review: ['i', 'really', 'like', 'this', 'show', '.', 'it', 'has', 'drama', ',', 'romance', ',', 'and', 'comedy', 'all', 'rolled', 'into', 'one', '.', 'i', 'am', '28', 'and', 'i', 'am', 'a', 'married', 'mother', ',', 'so', 'i', 'can', 'identify', 'both', 'with', 'lorelei', \"'\", 's', 'and', 'rory', \"'\", 's', 'experiences', 'in', 'the', 'show', '.', 'i', 'have', 'been', 'watching', 'mostly', 'the', 'repeats', 'on', 'the', 'family', 'channel', 'lately', ',', 'so', 'i', 'am', 'not', 'up-to-date', 'on', 'what', 'is', 'going', 'on', 'now', '.', 'i', 'think', 'females', 'would', 'like', 'this', 'show', 'more', 'than', 'males', ',', 'but', 'i', 'know', 'some', 'men', 'out', 'there', 'would', 'enjoy', 'it', '!', 'i', 'really', 'like', 'that', 'is', 'an', 'hour', 'long', 'and', 'not', 'a', 'half', 'hour', ',', 'as', 'th', 'hour', 'seems', 'to', 'fly', 'by', 'when', 'i', 'am', 'watching', 'it', '!', 'give', 'it', 'a', 'chance', 'if', 'you', 'have', 'never', 'seen', 'the', 'show', '!', 'i', 'think', 'lorelei', 'and', 'luke', 'are', 'my', 'favorite', 'characters', 'on', 'the', 'show', 'though', ',', 'mainly', 'because', 'of', 'the', 'way', 'they', 'are', 'with', 'one', 'another', '.', 'how', 'could', 'you', 'not', 'see', 'something', 'was', 'there', '(', 'or', 'take', 'that', 'long', 'to', 'see', 'it', 'i', 'guess', 'i', 'should', 'say', ')', '?', 'happy', 'viewing', '!']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display tokenized examples with labels for training dataset\n",
        "print(\"Training Dataset:\")\n",
        "for review, label in train_data[:3]:\n",
        "    print(\"Label:\", \"Positive\" if label == 1 else \"Negative\")\n",
        "    print(\"Tokenized Review:\", review)\n",
        "    print()\n",
        "\n",
        "# Display tokenized examples with labels for testing dataset\n",
        "print(\"Testing Dataset:\")\n",
        "for review, label in test_data[:3]:\n",
        "    print(\"Label:\", \"Positive\" if label == 1 else \"Negative\")\n",
        "    print(\"Tokenized Review:\", review)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW1iFsYaClk4"
      },
      "source": [
        "This script loads the IMDB dataset, tokenizes the reviews using the basic_english tokenizer, and displays tokenized examples for both positive and negative sentiment reviews.\n",
        "\n",
        "(3) Implement the Transformer Model:Implement the Transformer model using the torch.nn.Transformer module.\n",
        "\n",
        "(4)Train the Model:Define loss function and optimizer, and train the model on the training dataset.\n",
        "\n",
        "(5) Evaluate the Model:Evaluate the trained model on the testing dataset.\n",
        "\n",
        "(6) Calculate accuracy and other relevant metrics.\n",
        "\n",
        "Submission:Submit your implementation along with a brief report describing your model architecture, training procedure, evaluation results, and any insights gained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1st Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yag80x_Ctc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 0, Loss: 0.6790, Accuracy: 0.67\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 1, Loss: 0.5858, Accuracy: 0.72\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 2, Loss: 0.5149, Accuracy: 0.74\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 3, Loss: 0.4858, Accuracy: 0.75\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 4, Loss: 0.4697, Accuracy: 0.74\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 5, Loss: 0.4638, Accuracy: 0.74\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 6, Loss: 0.4290, Accuracy: 0.77\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 7, Loss: 0.4196, Accuracy: 0.78\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 8, Loss: 0.6297, Accuracy: 0.57\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "Epoch: 9, Loss: 0.6778, Accuracy: 0.51\n"
          ]
        }
      ],
      "source": [
        "# code helper\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Your code here: implement the Transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_encoder_layers, hidden_dim, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_size, \n",
        "            nhead=num_heads, \n",
        "            dim_feedforward=hidden_dim, \n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Set batch_first to True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.fc = nn.Linear(embed_size, 2)  # Assuming 2 classes for your output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # [batch_size, seq_len, embed_size]\n",
        "        x = x.transpose(0, 1)  # Transpose to [seq_len, batch_size, embed_size] for Transformer\n",
        "        x = self.transformer_encoder(x)  # [seq_len, batch_size, embed_size]\n",
        "        x = x.mean(dim=0)  # Average pooling over the sequence dimension [batch_size, embed_size]\n",
        "        x = self.fc(x)  # [batch_size, 2]\n",
        "        return x\n",
        "    \n",
        "# Define loss function and optimizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Flatten all token lists into a single list and count occurrences\n",
        "# all_train_tokens = [token for token_list, _ in train_data for token in token_list]\n",
        "# vocab_counter = Counter(all_train_tokens)\n",
        "# # The vocabulary size is the number of unique tokens\n",
        "# vocab_size = len(vocab_counter)\n",
        "\n",
        "embed_size = 128\n",
        "num_heads = 2\n",
        "num_encoder_layers = 2\n",
        "hidden_dim = 256\n",
        "dropout = 0.2\n",
        "\n",
        "# Your code here: define loss function and optimizer\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, num_encoder_layers, hidden_dim, dropout)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Build vocab from train data\n",
        "vocab = build_vocab_from_iterator((token_list for token_list, _ in train_data), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Helper function to encode and pad token lists\n",
        "def encode_and_pad(token_list, vocab, pad_index, max_length=None):\n",
        "    indices = [vocab[token] for token in token_list]\n",
        "    if max_length is None:\n",
        "        max_length = max(len(t) for t, _ in train_data)\n",
        "    padded_indices = indices + [pad_index] * (max_length - len(indices))\n",
        "    return padded_indices\n",
        "\n",
        "# Encode and pad the token lists from train and test data\n",
        "pad_index = vocab[\"<pad>\"]\n",
        "max_length = max(max(len(t) for t, _ in train_data), max(len(t) for t, _ in test_data))  # Get max length if needed\n",
        "\n",
        "train_encoded = [torch.tensor(encode_and_pad(token_list, vocab, pad_index, max_length), dtype=torch.long) for token_list, _ in train_data]\n",
        "train_labels = torch.tensor([label for _, label in train_data], dtype=torch.long)\n",
        "test_encoded = [torch.tensor(encode_and_pad(token_list, vocab, pad_index, max_length), dtype=torch.long) for token_list, _ in test_data]\n",
        "test_labels = torch.tensor([label for _, label in test_data], dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(torch.stack(train_encoded), train_labels)\n",
        "test_dataset = TensorDataset(torch.stack(test_encoded), test_labels)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32 # You can adjust the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "### BUG FIX\n",
        "max_index = max([max(seq) for seq in train_encoded if len(seq) > 0])\n",
        "if max_index >= vocab_size:\n",
        "    raise ValueError(f\"Index {max_index} out of range with vocab size {vocab_size}\")\n",
        "\n",
        "for token_list, _ in train_data:\n",
        "    for token in token_list:\n",
        "        index = vocab[token]\n",
        "        if index >= vocab_size:  # vocab_size is the size set in the embedding layer\n",
        "            print(f\"Out-of-range token '{token}' with index {index}\")\n",
        "### BUG FIX\n",
        "\n",
        "# Your code here: train the model\n",
        "# Train the model\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        if count % 100==0:\n",
        "            print(count)\n",
        "\n",
        "        count += 1\n",
        "        inputs, targets = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Your code here: evaluate the model\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            inputs, targets = batch\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Run training and evaluation\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    train_loss = train(model, train_loader)\n",
        "    accuracy = evaluate(model, test_loader)\n",
        "    print(f\"Epoch: {epoch}, Loss: {train_loss:.4f}, Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2nd Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Epoch: 01 | Train Loss: 0.484 | Train Acc: 75.44% | Val. Loss: 0.359 | Val Acc: 86.28%\n",
            "epoch 1\n",
            "Epoch: 02 | Train Loss: 0.257 | Train Acc: 89.84% | Val. Loss: 0.344 | Val Acc: 88.50%\n",
            "epoch 2\n",
            "Epoch: 03 | Train Loss: 0.171 | Train Acc: 93.67% | Val. Loss: 0.405 | Val Acc: 88.68%\n",
            "epoch 3\n",
            "Epoch: 04 | Train Loss: 0.112 | Train Acc: 96.20% | Val. Loss: 0.467 | Val Acc: 87.98%\n",
            "epoch 4\n",
            "Epoch: 05 | Train Loss: 0.074 | Train Acc: 97.61% | Val. Loss: 0.614 | Val Acc: 88.14%\n",
            "epoch 5\n",
            "Epoch: 06 | Train Loss: 0.050 | Train Acc: 98.40% | Val. Loss: 0.610 | Val Acc: 87.32%\n",
            "epoch 6\n",
            "Epoch: 07 | Train Loss: 0.033 | Train Acc: 98.96% | Val. Loss: 0.871 | Val Acc: 87.18%\n",
            "epoch 7\n",
            "Epoch: 08 | Train Loss: 0.022 | Train Acc: 99.30% | Val. Loss: 0.959 | Val Acc: 87.42%\n",
            "epoch 8\n",
            "Epoch: 09 | Train Loss: 0.015 | Train Acc: 99.39% | Val. Loss: 1.119 | Val Acc: 87.34%\n",
            "epoch 9\n",
            "Epoch: 10 | Train Loss: 0.014 | Train Acc: 99.56% | Val. Loss: 0.960 | Val Acc: 86.56%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "# Function to download and extract IMDB dataset\n",
        "def download_extract_imdb(root=\"./imdb_data\"):\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "\n",
        "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    filename = os.path.join(root, \"aclImdb_v1.tar.gz\")\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    # Extract the tar.gz file\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall(root)\n",
        "\n",
        "# Check if the dataset is downloaded and extracted\n",
        "if not os.path.exists(\"./imdb_data/aclImdb\"):\n",
        "    download_extract_imdb()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Load data\n",
        "def load_imdb_data(root=\"./imdb_data/aclImdb\"):\n",
        "    data = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        label_dir = os.path.join(root, \"train\", label)\n",
        "        for filename in os.listdir(label_dir):\n",
        "            with open(os.path.join(label_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                review = file.read()\n",
        "                # Tokenize review\n",
        "                tokenized_review = tokenizer(review)\n",
        "                data.append((tokenized_review, 1 if label == \"pos\" else 0))\n",
        "    return data\n",
        "\n",
        "# Load training data\n",
        "train_data = load_imdb_data()\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(data, unk_token=\"<unk>\", pad_token=\"<pad>\"):\n",
        "    vocab = set()\n",
        "    for tokens, _ in data:\n",
        "        vocab.update(tokens)\n",
        "    vocab = list(vocab)\n",
        "    vocab.insert(0, pad_token)  # padding token\n",
        "    vocab.insert(0, unk_token)  # unknown token\n",
        "    vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab_to_idx, vocab\n",
        "\n",
        "vocab_to_idx, vocab = build_vocab(train_data)\n",
        "\n",
        "# Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "        pooled_output = torch.mean(transformer_output, dim=0)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VOCAB_SIZE = len(vocab_to_idx)\n",
        "EMBED_DIM = 60\n",
        "NUM_HEADS = 2\n",
        "HIDDEN_DIM = 60\n",
        "NUM_LAYERS = 1\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "model = TransformerModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS, NUM_CLASSES).to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# Define the collate function to pad sequences\n",
        "def collate_batch(batch):\n",
        "    text, labels = zip(*batch)\n",
        "    labels = torch.tensor(labels)\n",
        "    # Find the maximum length of text in the batch\n",
        "    max_length = max(len(item) for item in text)\n",
        "    # Create a tensor to hold the padded sequences\n",
        "    padded_text = torch.zeros((len(text), max_length), dtype=torch.long)\n",
        "    for i, item in enumerate(text):\n",
        "        # Fill the tensor with the sequences, leaving the remaining space as padding\n",
        "        padded_text[i, :len(item)] = torch.tensor([vocab_to_idx[token] for token in item])\n",
        "    return padded_text, labels\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "# Create data loaders\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Define the number of epochs\n",
        "N_EPOCHS = 10\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    # Training Phase\n",
        "    print(\"epoch\", epoch)\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_train_loss = 0\n",
        "    correct_train = 0  # Initialize correct prediction counter for training\n",
        "\n",
        "    for text, labels in train_iterator:\n",
        "        text, labels = text.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(text)\n",
        "\n",
        "        # Compute the loss\n",
        "        train_loss = criterion(predictions, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        train_loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the training loss for this batch\n",
        "        epoch_train_loss += train_loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(predictions.data, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    # Compute average training loss and accuracy for the epoch\n",
        "    average_train_loss = epoch_train_loss / len(train_iterator)\n",
        "    train_accuracy = 100 * correct_train / len(train_iterator.dataset)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    epoch_val_loss = 0\n",
        "    correct_val = 0  # Initialize correct prediction counter for validation\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during validation\n",
        "        for text, labels in val_iterator:\n",
        "            text, labels = text.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(text)\n",
        "\n",
        "            # Compute the loss\n",
        "            val_loss = criterion(predictions, labels)\n",
        "\n",
        "            # Accumulate the validation loss for this batch\n",
        "            epoch_val_loss += val_loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(predictions.data, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    # Compute average validation loss and accuracy for the epoch\n",
        "    average_val_loss = epoch_val_loss / len(val_iterator)\n",
        "    val_accuracy = 100 * correct_val / len(val_iterator.dataset)\n",
        "\n",
        "    # Print epoch information\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {average_train_loss:.3f} | Train Acc: {train_accuracy:.2f}% | Val. Loss: {average_val_loss:.3f} | Val Acc: {val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **BRIEF REPORT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformer-Based Models for Sentiment Analysis:\n",
        "\n",
        "\n",
        "**Model 1:**\n",
        "\n",
        "- Architecture: Multi-head encoding, manual token handling and vocabulary management.\n",
        "- Training Time: Extensive, around 9 hours.\n",
        "- Performance: Achieved low accuracy (51%) with high training time, indicating inefficiency.\n",
        "- Insight: The substantial training duration of approximately 9 hours and a drop in accuracy from 77% to 51% in later epochs for Model 1 emphasize the inefficiencies in its manual data preprocessing approach and the need for more streamlined data management techniques to optimize performance.\n",
        "\n",
        "\n",
        "**Model 2:**\n",
        "\n",
        "- Architecture: Simplified and efficient, utilizing built-in functions for data handling and model operations, multi-head encoding within its Transformer architecture. Streamlines data handling by using a custom collate function to manage padding and batch creation efficiently.\n",
        "- Training Time: Significantly shorter, around 30 minutes.\n",
        "- Performance: High initial accuracy (up to 88%) but showed signs of overfitting as indicated by increasing validation loss despite improving training accuracy.\n",
        "- Insight: Model 2’s efficiency is evident with a training time of approximately 34 minutes and an impressive peak accuracy of 88.68%. However, the rise in validation loss during later epochs highlights the need for adjustments in model regularization to prevent overfitting.\n",
        "\n",
        "The significant difference in training times between the two transformer models is mainly due to Model 1's manual data processing and complex architecture, which increase computational overhead and extend training duration. In contrast, Model 2 uses optimized torchtext functions for efficient data handling and a simpler architecture, along with a smaller batch size, enhancing training speed, thereby substantially reducing its training time. \n",
        "\n",
        "Model 2 is preferable for practical applications due to its efficiency and simplicity, though it may require adjustments to prevent overfitting, such as fine-tuning dropout rates or epoch numbers. Model 1, while detailed and informative for understanding model internals, needs optimization to reduce training time and improve stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrEXBFw4C3Gw"
      },
      "source": [
        "**Submission Instructions:**\n",
        "\n",
        "Submit your Python code in a single notebook file, show your work in detail."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
